id,title,selftext,score,num_comments,author,created_utc,url,over_18,edited,spoiler,stickied
1j57mru,Fabric sucks but itâ€™s what the people want,"What the title says. Fabric sucks. Itâ€™s an incomplete solution. The UI is muddy and not intuitive. Microsoftâ€™s previous setup was better.
But since theyâ€™re moving PowerBI to the service companies have to move to Fabric. 
It may be anecdotal but Iâ€™ve seen more companies look specifically for people with Fabric experience. 
If youâ€™re on the job hunt Iâ€™d look into getting Fabric experience. Companies who havenâ€™t considered cloud are now making the move because they already use Microsoft products, so Microsoft is upselling them to the cloud. I could see Microsoft taking the top spot as a cloud provider soon. 
This is what Iâ€™ve seen in the US.",82,58,hijkblck93,2025-03-06 22:10:06,https://www.reddit.com/r/dataengineering/comments/1j57mru/fabric_sucks_but_its_what_the_people_want/,False,False,False,False
1j5jbcm,"When the database is fine, but you're not ðŸ¤¯",,91,4,Adela_freedom,2025-03-07 09:03:59,https://i.redd.it/k0vpe5jlg8ne1.png,False,False,False,False
1j4worb,People who joined Big Tech and found it disappointing... What was your experience?,"I came across the question on r/cscareerquestions and wanted to bring it here. For those who joined Big Tech but found it disappointing, what was your experience like?

Original Posting: [https://www.reddit.com/r/cscareerquestions/comments/1j4mlop/people\_who\_joined\_big\_tech\_and\_found\_it/](https://www.reddit.com/r/cscareerquestions/comments/1j4mlop/people_who_joined_big_tech_and_found_it/)

Would a Data Engineer's experience would differ from that of a Software Engineer? 

Please include the country you are working from, as experiences can differ greatly from country to country. For me, I am mostly interested in hearing about US/Canada experiences.

To keep things a little more positive, after sharing your experience, please include one positive (or more) aspect you gained from working at Big Tech that wasnâ€™t related to TC or benefits.

Thanks!",64,37,PotatoInTheStars,2025-03-06 14:29:20,https://www.reddit.com/r/dataengineering/comments/1j4worb/people_who_joined_big_tech_and_found_it/,False,False,False,False
1j5bttx,SQLMesh versus dbt Core - Seems like a no-brainer,"I am familiar with dbt Core. I have used it. I have written tutorials on it. dbt has done a lot for the industry. I am also a big fan of SQLMesh. Up to this point, I have never seen a performance comparison between the two open-core offerings. Tobiko just released a benchmark report, and I found it super interesting. TLDR - SQLMesh appears to crush dbt core. Is that anyone elseâ€™s experience?

Hereâ€™s the report link - [https://tobikodata.com/tobiko-dbt-benchmark-databricks.html](https://tobikodata.com/tobiko-dbt-benchmark-databricks.html)

**Here are my thoughts and summary of the findings -**

I found the technical explanations behind these differences particularly interesting.

The benchmark tested four common data engineering workflows on Databricks, with SQLMesh reporting substantial advantages:

\- Creating development environments: 12x faster with SQLMesh

\- Handling breaking changes: 1.5x faster with SQLMesh

\- Promoting changes to production: 134x faster with SQLMesh

\- Rolling back changes: 136x faster with SQLMesh

According to Tobiko, these efficiencies could save a small team approximately 11 hours of engineering time monthly while reducing compute costs by about 9x. Thatâ€™s a lot.

**The Technical Differences**

The performance gap seems to stem from fundamental architectural differences between the two frameworks:

SQLMesh uses virtual data environments that create views over production data, whereas dbt physically rebuilds tables in development schemas. This approach allows SQLMesh to spin up dev environments almost instantly without running costly rebuilds.

SQLMesh employs column-level lineage to understand SQL semantically. When changes occur, it can determine precisely which downstream models are affected and only rebuild those, while dbt needs to rebuild all potential downstream dependencies. Maybe dbt can catch up eventually with the purchase of SDF, but it isnâ€™t integrated yet and my understanding is that it wonâ€™t be for a while.

For production deployments and rollbacks, SQLMesh maintains versioned states of models, enabling near-instant switches between versions without recomputation. dbt typically requires full rebuilds during these operations.

**Engineering Perspective**

~~As someone who's experienced the pain of 15+ minute parsing times before models even run in environments with thousands of tables, these potential performance improvements could make my life A LOT better.~~ I was mistaken (see reply from Toby below). The benchmarks are RUN TIME not COMPILE time. SQLMesh is crushing on the run. I misread the benchmarks (or misunderstood...I'm not that smart ðŸ˜‚) 

https://preview.redd.it/3580t9hmy6ne1.jpg?width=1546&format=pjpg&auto=webp&s=bb66f09111d3f1a7acf08fd23031fa07821df8fd

However, I'm curious about real-world experiences beyond the controlled benchmark environment. SQLMesh is newer than dbt, which has years of community development behind it.

Has anyone here made the switch from dbt Core to SQLMesh, particularly with Databricks? How does the actual performance compare to these benchmarks? Are there any migration challenges or feature gaps I should be aware of before considering a switch?

Again, the benchmark report is available here if you want to check the methodology and detailed results: [https://tobikodata.com/tobiko-dbt-benchmark-databricks.html](https://tobikodata.com/tobiko-dbt-benchmark-databricks.html)",53,30,Andrew_Madson,2025-03-07 01:24:58,https://www.reddit.com/r/dataengineering/comments/1j5bttx/sqlmesh_versus_dbt_core_seems_like_a_nobrainer/,False,False,False,False
1j4wzpj,OpenMetadata and Python models,"Hii, my team and I are working around how to generate documentation for our python models (models understood as Python ETL).

We are a little bit lost about how the industry are working around documentation of ETL and models. We are wondering to use Docstring and try to connect to OpenMetadata (I don't if its possible).

Kind Regards.",17,28,thejosess,2025-03-06 14:43:15,https://www.reddit.com/r/dataengineering/comments/1j4wzpj/openmetadata_and_python_models/,False,False,False,False
1j5j59f,How do you handle data schema evolution in your company?,"You know data schemas change, they grow, they shrink, and sometimes in a backward incompatible way. 

What how do you handle it? do you use like Iceberg? or do you try to reduce the change in the first place? etc",21,16,Substantial_Lab_5160,2025-03-07 08:51:03,https://www.reddit.com/r/dataengineering/comments/1j5j59f/how_do_you_handle_data_schema_evolution_in_your/,False,False,False,False
1j55dk3,Is data engineering a lost cause in Australia ?,"I have been pursuing a data engineer career for the last 6 years. I am in a situation where there are no data engineer roles in Canberra. I am looking for a data role with flair or ETL and Power BI in an outside Canberra organisation.
",16,9,Antique_Reporter6217,2025-03-06 20:34:52,https://www.reddit.com/r/dataengineering/comments/1j55dk3/is_data_engineering_a_lost_cause_in_australia/,False,False,False,False
1j5260t,"CentralMind/Gateway - Open-Source AI-Powered API generation from your database, optimized for LLMs and Agents","Weâ€™re building an open-source tool -Â [https://github.com/centralmind/gateway](https://github.com/centralmind/gateway)Â that makes it easy to generate secure, LLM-optimized APIs on top of your structured data without manually designing endpoints or worrying about compliance.

AI agents and LLM-powered applications need access to data, but traditional APIs and databases werenâ€™t built with AI workloads in mind. Our tool automatically generates APIs that:

\- Optimized for AI workloads, supporting Model Context Protocol (MCP) and REST endpoints with extra metadata to help AI agents understand APIs, plus built-in caching, auth, security etc.

\- Filter out PII & sensitive data to comply with GDPR, CPRA, SOC 2, and other regulations.

\- Provide traceability & auditing, so AI apps arenâ€™t black boxes, and security teams stay in control.

Its easyÂ [to connect as custom action in chatgpt](https://docs.centralmind.ai/docs/content/integration/chatgpt/)Â or in Cursor, Cloude Desktop as MCP tool with just few clicks.

https://reddit.com/link/1j5260t/video/t0fedsdg94ne1/player

We would love to get your thoughts and feedback! Happy to answer any questions.",13,8,Gaploid,2025-03-06 18:21:42,https://www.reddit.com/r/dataengineering/comments/1j5260t/centralmindgateway_opensource_aipowered_api/,False,False,False,False
1j5ekge,"If you were suddenly in charge of creating a data engineering foundation for a startup, what would your first 3 months look like?","So I'm not a data engineer, I'm a data analyst. The only problem is, I'm possibly being brought into a 4 month old start up, they're enthusiastic but have little idea what they're doing data wise. They admitted as much, and if I join the company I would be the most technical person on deck.

Since I'm an analyst having to create everything from the ground up would be a challenge for me. Granted, I have worked on data architecture and data engineering processes in the past, I know how to set up ETLs etc. But usually in a team setting, where someone else already came up with the schematics for me to build around. This time it'll just be me building so that I can conduct analysis. If you were in my shoes, and you wanted to prove value in your first 3 months, how would you go about it?",12,16,Double_Education_975,2025-03-07 03:48:35,https://www.reddit.com/r/dataengineering/comments/1j5ekge/if_you_were_suddenly_in_charge_of_creating_a_data/,False,False,False,False
1j537f1,Golly do I Feel Inadequate,"Hey, long-time imposter syndrome thread reader and first-time poster here.

The good news. After doing both a bachelors and masters in STEM, and working in industry for about 7 years I've landed a job in my dream industry as a data engineer. It's been a dream industry for me since I was a teenager. It's a startup company, and wow is this way different than working for a big company. I'm 9 working days in, and I've got a project to complete in the matter of 20 days. Not like a big company, where the expectation was that I know where the bathroom is after 6 months.

The bad news. For the longest time, I thought I wanted to be a data scientist and heart I probably still do. So I worked in roles that let me build models and do mathy things. However after multiple years of trying, my dream industry seemed like it didn't want me as data scientist. Probably because I don't really care for deep learning.  I heard a quote recently that goes ""if you get a seat on a rocket ship don't worry about what seat it is."" As it turns out my seat on the rocket ship is being a data engineer. In previous roles I did data engineering-ish things. Lots of SQL and pyspark, and using APIs to get data. But now being at a start up, the responsibilities seem way broader. Delving deep into the world of Linux and bash scripting, Docker, and async programming all of which I've really never actually touched until now.

Come to find out one the reasons I was hired was because of my passion for the industry, and that I have just enough technical knowledge to not look like a buffoon. Some of the people on my team are contractors, that don't have a clue about what industry they're working in. I've managed to be a mentor to them in my short 9 days. That said, they could wipe the floor with me on the technical side. They're over there using fancy things like GitHub actions and pydantic, and type hints.

It's very much been trial by fire on this project I'm on. I wrote a couple functions, and someone basically took the reigns to refactor that into something Airflow can use. And now it's my turn to try and actually orchestrate and deploy the damn thing.

In my experience project based learning has taught me plenty but, the learning curve is always steep especially when it's in industry and not some small personal thing.

I don't know about you but for me, most docs for python libraries are dense and don't make anything clearer when you've never actually used that tool before. I know there's loads of YouTube videos and books  but, let's be honest only some of those are actually worthwhile.

So my questions to you, the reader of this thread, what resources do you recommend for a data engineer just now getting their feet wet? Also how the hell do you deal with your feelings of inadequacy?",9,6,JD_ThrowAway_1738,2025-03-06 19:04:06,https://www.reddit.com/r/dataengineering/comments/1j537f1/golly_do_i_feel_inadequate/,False,False,False,False
1j5iy29,Databricks Custom Data Source â€” Practical Examples,"Python Data Source API in Spark 4.0, Databricks allows the integration of custom data sourcesâ€”I found it useful to read from REST APIs or to generate synthetic data, and improve cluster utilization.  
Benefits:  
âœ… Easy Custom REST API Integration  
âœ… Fast Fake Data Generation for Testing  
âœ… Optimized Cluster Utilization Using Partition  
  
Link to my post:

[https://medium.com/@mariusz\_kujawski/databricks-custom-data-source-practical-examples-e534b23c1fa7](https://medium.com/@mariusz_kujawski/databricks-custom-data-source-practical-examples-e534b23c1fa7)",8,0,4DataMK,2025-03-07 08:35:39,https://www.reddit.com/r/dataengineering/comments/1j5iy29/databricks_custom_data_source_practical_examples/,False,False,False,False
1j51bb2,Need help with deploying Dagster,"Hey folks. For some context, Iâ€™ve been working as a data engineer for about a year now. 

The team Iâ€™m on is primarily composed of analysts and data engineers whose only experience is in Informatica. Around the time I joined my organization, the team decided to start transitioning to Python based data pipelines and chose Dagster as the orchestration service. 

Now, since Iâ€™m the only one with any tangible skills in Python, the entire responsibility of developing, testing, deploying and maintaining our pipelines has fallen on me. While I do enjoy the freedom and many learning opportunities it grants me, Iâ€™m smart enough to realize the downsides of not having a more experienced engineer offer their guidance. 

Right now, the biggest problem Iâ€™m facing is with how to best set up my Dagster projects and how to deploy them efficiently, keeping in mind my teams specific requirements and also some other setup related things surrounding this. Iâ€™d also greatly appreciate some mentoring and guidance in general when it comes to Dagster and data engineering best practices in the industry, since I have no one to turn to at my own organization. 

So, if youâ€™re an experienced data engineer and donâ€™t mind being a mentor and lettting me pick your brain about these things, please do leave a comment and Iâ€™ll DM you with more details about what Iâ€™m trying to solve. 

Thanks in advance. Cheers. 

Edit: Fixed some weird grammar",4,9,YameteGPT,2025-03-06 17:46:57,https://www.reddit.com/r/dataengineering/comments/1j51bb2/need_help_with_deploying_dagster/,False,False,False,False
1j5lmfe,How do you deal with multi-modal (image + metadata) data in your reporting pipelines?,"My organisation in the company is coming up with several projects utilising images + metadata associated with each image. Each image is 10+mb, and they need to go through some filtering, pre-processing and resizing and matching images to their metadata. 

Resulting data would be used for labelling and ML but they also want to be able to view images filtering with the metadata attributes. 

  
We deploy pipelines in Azure Databricks and mainly use PowerBI as a front-end. Last time I tried to load images into a Delta table it performed really poorly. I'm also not confident in PowerBI to handle a lot of images. 

  
How do you deal with such use cases or what would you do in this scenario?",7,7,paustic,2025-03-07 11:44:35,https://www.reddit.com/r/dataengineering/comments/1j5lmfe/how_do_you_deal_with_multimodal_image_metadata/,False,False,False,False
1j542hv,Data Quality and Data Validation in Databricks,"Hi,

I want to create a Data Validation and Quality checker in my Databricks workflow  as I have a ton of data pipelines and I want to flag out any issues.

I was looking at Great Expectations but oh my god it's so cumbersome, it's been a day and I still haven't figured it out. Also, their documentation on the Databricks section seems to be outdated in some portions.

Can someone help me with what can be a good way to do this?
Honestly I felt like giving up and writing my own functions and trigger emails in case something goes off.

I know it won't be very scalable and will need intervention and documentation, but I can't seem to find a solution to this.",4,6,suffer-surfer,2025-03-06 19:40:17,https://www.reddit.com/r/dataengineering/comments/1j542hv/data_quality_and_data_validation_in_databricks/,False,False,False,False
1j4wd6f,Building a real-time data pipeline for employee time tracking & scheduling (hospitality industry),"Hi everyone, I am a Fresher Data Engineer, I have around-a-year experience as a Data Analyst. 

Iâ€™m working on a capstone project aimed at solving a real-world problem in the restaurant industry: effectively tracking employee work hours and comparing them with planned schedules to identify overtime and staffing issues (This project hasn't been finished yet but I desire to post here to learn from our community' feedbacks and suggestions).

I am intending to improve this project to make it comprehensive and then use it for my portfolio project in terms of looking for a job.

FYI: I am actually still learning Python everyday, but TBH with the help of chatGPT (or Grok), it helps me to code, to detect bugs, and to maintain the nice scripts for this project.

**Project Overview:**

\- Tracks real-time employee activity: Employees log in and out using a web app deployed on tablets at each restaurant location.

\- Stores event data: Each login/logout event is captured as a message and sent to a Kafka topic.

\- Processes data in batches: A Kafka consumer (implemented in Python) retrieves these messages and writes them to a PostgreSQL database (acting as a data warehouse). We also handle duplicate events and late-arriving data. (actually the data volume coming from login/logout event is not that big to use Kafka message but I want to showcase my ability to use batch processing and streaming process if necessary, basically I use psycopg2 connection to insert data into local PostgreSQL database)

\- Calculates overtime: Using Airflow, we schedule ETL jobs that compare actual work hours (from the logged events) with planned schedules.

\- Manager UI for planned schedules: A separate Flask web app enables managers to input and view planned work schedules for each employee. The UI uses dropdown menus to select a location (e.g., US, UK, CN, DEN, FIN ...) and dynamically loads the employees for that location (I have an employee database where it stores all necessary information about each employee), then displays an editable table for setting work hours.

**Tools & Technologies Used:**

Flask: Two separate applicationsâ€”one for employee login/logout and one for manager planned schedule input. (For frontend application, I often communicate with ChatGPT to build the basic layout and interactive UI such as .HTML file)

Kafka: Used as the messaging system for real-time event streaming (with Dockerized Kafka & Zookeeper).

Airflow: Schedules batch processing/ETL jobs to process Kafka messages and compute overtime.

PostgreSQL: Acts as the main data store for employee data, event logs (actual work hours), and planned schedules.

Docker: Used to containerize Kafka, Airflow, and other backend services.

Python: For scripting the consumer, ETL logic, and backend services.

\-------------------------------------

I would love to hear your feedback on this pipeline. Is this architecture practical for a real-world deployment? What improvements or additional features would you suggest? Are there any pitfalls or alternative approaches that I should consider to make this project even more robust and scalable? THANK YOU EVERYONE, I apologize if this post is too long for everyone but I am new to data engineering so my project explanation is a bit clumsy and wordy.  
",3,5,henryhai0407,2025-03-06 14:14:22,https://www.reddit.com/r/dataengineering/comments/1j4wd6f/building_a_realtime_data_pipeline_for_employee/,False,False,False,False
1j5lgzr,Seeking Advice on Fast Data Exploration for Billions of Rows,"Hi everyone,

I have a database with billions of rows and need a fast, efficient way to explore the data. Currently, using Tableau's Hyper works well up to a point, but beyond a certain data volume, filters become noticeably slow.

I recently tested the dataset with DuckDB and saw very promising results in terms of query performance. However, for non-technical users, I want to build an interfaceâ€”similar to a dashboard with tables and filters like in Tableauâ€”for interactive data exploration.

Iâ€™m considering using Streamlit to display tables and applying filters on parts of the data via DuckDB. My concern is that, based on my research, I might have to convert visualizations to pandas DataFrames before sending them to Streamlit, which could limit scalability.

Also, I donâ€™t want to use any cloud solutions.

What are your suggestions for addressing this challenge? Is there any open-source tool or alternative stack that youâ€™ve found effective for fast data exploration on such large datasets?

Thanks in advance for your insights!",3,5,Annual_Elderberry541,2025-03-07 11:34:37,https://www.reddit.com/r/dataengineering/comments/1j5lgzr/seeking_advice_on_fast_data_exploration_for/,False,False,False,False
1j5immi,[Advice Needed] Getting Started in Data Engineering: Approach and Productionizing Pipelines,"Hi everyone,

I recently graduated with a **Masterâ€™s in Business Intelligence**, during which I worked as a **Data Scientist in an apprenticeship**. I gained experience in **Machine Learning, Deep Learning, and Data Engineering**, including:

* **Data Mart / Data Warehouse modeling** (star schema, snowflake schema, SCDâ€¦)
* **Developing ETL pipelines** with **Talend** (staging â†’ transformation â†’ storage)
* **Data manipulation and transformation with Python**

I have a **strong background in Python** and have worked on standard data processing workflows (extraction, transformation, cleaning).



# Context of My Data Engineering Mission

Letâ€™s say I join a company that has **no existing data infrastructure**, apart from Excel files and some manual reports. The goal would be to set up a **data management system** to feed **Power BI dashboards**.

Based on my research, the project would involve the following steps:

1. **Gather requirements**: Define the **KPIs**, data sources, update frequencies, granularity, and quality rules.
2. **Design a Data Mart** tailored to reporting needs.
3. **Develop a data pipeline** to extract and transform data (from an ERP, CSV/Excel files, APIsâ€¦).
4. **Store the data** in a structured manner (in an SQL database or a Data Warehouse).
5. **Create visualizations** in Power BI.
6. **Automate and orchestrate** the pipeline (later, possibly using Airflow or another tool).

For now, I am focusing on setting up the **initial pipeline in Python**, which will process **CSV files placed in a folder** or data **from an ERP**, for example.



# My Questions About Productionization

I realize that while I know how to **clean and transform data**, I have never been taught how to **deploy a data pipeline in production** properly.

1. **Pipeline Automation**
   * If I need to process **manually placed CSV files**, what is the **best approach** for automating their ingestion?
   * I considered using **watchdog (Python) to detect a new file** and trigger the pipeline, but is this a good practice?
   * An alternative would be to **load these files directly into an SQL database** and process them there. What do you think?
2. **Orchestration and Industrialization**
   * At what point should one move from a **simple Python script + cron job** to **Airflow orchestration**?
   * Is using **Docker and Kubernetes** relevant from the start, or only in more advanced infrastructures?
   * If scaling is needed later, what **best practices** should be implemented from the beginning?
3. **Error Handling and Monitoring**
   * How do you **handle errors and ensure traceability** in your pipelines in a professional setting? (Logging, alerts, retry mechanismsâ€¦)
   * Are there any **recommended Python frameworks** for standardizing a data pipeline?
4. **DevOps, DataOps, and MLOps**
   * Does my need for industrialization fall more under **DevOps** or **DataOps**?
   * Do you have any **practical advice or resources** for learning these concepts effectively?

  
I would like to **validate my approach** and **avoid common mistakes** in Data Engineering. Iâ€™ve seen different solutions on this topic, but Iâ€™d love to hear from **professionals who have implemented similar projects**.

If you have any **resources, best practices, or real-world examples**, I would really appreciate your insights.

Thanks in advance for your help.",2,1,roblai_,2025-03-07 08:11:28,https://www.reddit.com/r/dataengineering/comments/1j5immi/advice_needed_getting_started_in_data_engineering/,False,False,False,False
1j58jjp,Synapse Link to Snowflake Loading Process,"I'm new to the DE world and stumbled into a role where I've taken on building pipelines when needed, so I'd love if someone could explain this like I'm an advanced 5 yr old. I'm learning from the firehose but do have built some super basic pipelines and good understanding of databases, so I'm not totally useless!

We are on D365 F&O and use a Synapse Link/Azure BLOB storage/Fivetran/Snowflake stack to get our data into a snowflake database. I would like to sync a table from our Test environment however there isn't the appetite to increase out monthly MAR in Fivetran the $1k for this test table, but I've been given the green light to make my own pipeline.

I have an external stage to the Azure container and see all the batch folders with the table I need, however I'm not quite sure how to process the changes.

  
Does anyone have any experience building pipelines from Azure to Snowflake using the Synapse Link folder structure?",3,2,Apprehensive-Ad-80,2025-03-06 22:50:12,https://www.reddit.com/r/dataengineering/comments/1j58jjp/synapse_link_to_snowflake_loading_process/,False,False,False,False
1j56pky,smallpond ... distributed DuckDB?,,2,0,averageflatlanders,2025-03-06 21:31:07,https://dataengineeringcentral.substack.com/p/smallpond-distributed-duckdb,False,False,False,False
1j5ocs1,No progression - working as a solo data engineer,"Hi , I need career advice. I am working as a data engineer for a gaming studio in Germany. I have 4 years of professional experience out of which 1 year is in purely data engineering. I work as the solo data engineer so there is no senior to learn from. Also the progression is pretty unclear in this company as a data engineer. I am also not passionate about gaming at all so I feel pretty isolated from the conversations that most people have at the company. 

Here, I work on maintaining a custom c# data pipeline, writing new jobs into the data pipeline, work with big query for data warehousing, looker studio for reporting, elastic search transforms for data analysis , data manipulation and analysis using sql queries etc . 

Currently the job market in Germany is really bad. But I want to change my job as Iâ€™m pretty unsatisfied at my current company. I tried forcing myself to get into gaming but I just couldnâ€™t . Would like to hear your thoughts on what I should do 
",6,1,areeba_k84,2025-03-07 13:54:27,https://www.reddit.com/r/dataengineering/comments/1j5ocs1/no_progression_working_as_a_solo_data_engineer/,False,False,False,False
1j5o9a1,Trouble Connecting SQL Server to Airflow in Docker â€“ Provider Not Recognized,"Iâ€™ve been trying to connect SQL Server to Apache Airflow running in Docker for the past two days, but I keep running into an issue where my DAG throws an error saying that the provider doesnâ€™t exist or isnâ€™t recognized.

Iâ€™ve installed the necessary providers (apache-airflow-providers-mssql) and added the necessary imports to my .py, but Airflow still doesnâ€™t seem to acknowledge it. Has anyone successfully connected SQL Server to Airflow in a Docker setup? If so, how did you do it?

The main goal is to schedule and execute SQL scripts. If Airflow is too much hassle for this, would it make more sense to just use a Python script to run the queries on a schedule instead? Open to suggestions.

Thanks in advance!",2,1,Macandcheeseilf,2025-03-07 13:50:36,https://www.reddit.com/r/dataengineering/comments/1j5o9a1/trouble_connecting_sql_server_to_airflow_in/,False,False,False,False
1j5o32j,Azure Certification at Half The Price,"Hello everyone! I just want to share that Microsoft will be holding a **Virtual Training Day** on March **11 and 12**

This is a **free event**, and by attending, you'll be eligible for a **50% discount** on the **Microsoft AZ-900 exam**.

Don't miss this opportunity! ðŸŒŸ

**Study Guide:** [https://learn.microsoft.com/credentials/certifications/resources/study-guides/az-900?wt.mc\_id=studentamb\_367268](https://learn.microsoft.com/credentials/certifications/resources/study-guides/az-900?wt.mc_id=studentamb_449330)

[link](https://msevents.microsoft.com/event?id=2845269515&wt.mc_id=eventscatalog)",4,1,ryanwolfh,2025-03-07 13:43:49,https://www.reddit.com/r/dataengineering/comments/1j5o32j/azure_certification_at_half_the_price/,False,False,False,False
1j5frtt,Redshift read operation while write lock,"I am trying to perform a query optimization task on a very big select query that utilizes more than 25 tables all present in redshift. All these 25 tables have etl processes running on them where some of these tables are locked every 5 minutes for loading processes. The select query has to run every 6 hour but since a lot of tables have locks on them, in most cases my query times out. 

I was wondering if redshift provides a feature like this- ""lock the table for writing new inserts, but meanwhile, let a select query read the stale data"" given i don't really mind reading the stale data, its an OLAP operation. I tried to search it up but i just seem to be unable to. Please let me know if anyone has any idea of this, its very critical for our business needs.",2,8,Academic_Meal_1742,2025-03-07 04:58:39,https://www.reddit.com/r/dataengineering/comments/1j5frtt/redshift_read_operation_while_write_lock/,False,False,False,False
1j54qld,Feedback on Snowflake's Declarative DCM,"Im looking for feedback for anyone that is using snowflakes new declarative DCM. This approach sounds great on paper, but also seems to have some big limitations. But Im curious what your experience has been. How does it compare to some of the imperative tools out there? Also, how does it compare to snowddl?

It seems like snowflake is pushing this forward and encouraging people to use it, and Im sure there will be improvements with it in the future. So I would like to use this approach if possible.

But right now, I am curious how others are handling the instances where create or alter is not supported. For example column or object renaming. Or altering the column data type? How do you handle this. Is this still a manual process that must be run before the code is deployed?",2,1,BeardedYeti_,2025-03-06 20:08:26,https://www.reddit.com/r/dataengineering/comments/1j54qld/feedback_on_snowflakes_declarative_dcm/,False,False,False,False
1j4zo4h,"Using EXCEPT, always the right way to compare?","Im working on a decommissioning project, task was to implement the altered workflows on Tableau.

I used tableau cloud, the row count was correct. Is using except function the right way to compare data,( outputs of alteryx and tableau prep)?


So Iâ€™m using EXCEPTALL in pyspark by comparing the output csv files.",2,5,NefariousnessSea5101,2025-03-06 16:39:22,https://www.reddit.com/r/dataengineering/comments/1j4zo4h/using_except_always_the_right_way_to_compare/,False,False,False,False
1j5jxdp,Suitable persistent tech stack for high storage and infrequent access,"Hi,  
Not sure if this is the right channel to post but please do suggest if you can help.

Could someone suggest suitable tech stack for the following usage - I want to create some usage dashboards from the data currently in my dynamodb (ready to migrate from the same). The dashboads are for stakeholders, to just observe. I considered s3(since dynamo has periodic ttl and we need historical data too, if they feel like looking at it)+graphana metrics but if someone deletes the s3 upload, I am screwed. Can someone suggest what data storage solution I should be looking at here with minimal expense?",1,1,Lecture_Tight,2025-03-07 09:46:29,https://www.reddit.com/r/dataengineering/comments/1j5jxdp/suitable_persistent_tech_stack_for_high_storage/,False,False,False,False
1j5joxh,Python Learning,Just started learning Python and data languages. Anyone keen to learn together? Please share your thoughts. ,1,1,Difficult-Range3426,2025-03-07 09:31:35,https://www.reddit.com/r/dataengineering/comments/1j5joxh/python_learning/,False,False,False,False
1j5ny3m,DBT warning,"Guys, can anyone tell me how I can stop DBT from showing warnings in the terminal?",0,1,Zagann_BR,2025-03-07 13:38:29,https://www.reddit.com/r/dataengineering/comments/1j5ny3m/dbt_warning/,False,False,False,False
1j5kmt6,Any Advice on a Complicated Calculation?,"Hi everyone, I have really been breaking my brain trying to figure out how to generate a particular outcome.

**IN A NUTSHELL**: I am trying to generate a query that generates a list of Stock Codes and the number of days over the past 45 days where they were IN STOCK (ie Stock Level >0).

**DISCLAIMER**: Before I go any further, please know that I am not a developer. I am an Entrepreneur and due to my company's growth I have had to jump into learning some SQL basics to keep my operations going. Everything I know, I have self-researched and I apologize in advance for my lack of proper terminology, over-explanation, or noob assumptions. I am self-taught on MS Access and use this, as it has always been the easiest and most accessible for me to use. I will do my best to present as much information in as much specific detail as possible!

**CONTEXT**: I am trying to calculate an accurate sell-out rate on the items I sell in my business over the past 45 days. Unfortunately, this is not a simple Sales / 45 calculation. I often have stock shortages due to supply-side issues. This causes the sell-out rate to skew, making these products look less popular than they really are (since they don't sell when there is no stock). My idea is to calculate how many days these items were in stock and thus generate a more accurate sell-out rate calculation (Sales / Last 45 Days in Which Stock Level >0). This is important for sales forecasting, stock holding etc.  
My stock system currently has **7,449** unique stock codes  
**STK\_STOCKTRANSACTION** has **349,285** lines

**MY ATTEMPT**: Doing some research and working with chatGPT I have come up with this:

**Tables**

1. **tblNumbers** \- a manual table of numbers from 1 - 45. 
   1. *Columns*: 
      1. ID: AutoNumber
      2. n: Number
2. **STK\_STOCKTRANSACTION** \- a linked table to my inventory system. This table logs all stock transactions. 
   1. *Columns*:
      1. STOCKCODE: Item stock code
      2. TRANDTETME: Transaction Date
      3. TRANSACTIONTYPE: Transaction Type. All transaction types are relevant for stock level calculation EXCEPT  ""INC CP"", ""CSTCNG"", ""DEC CP"", ""CPDIFF""
      4. QTY1: Quantity

**Queries**

1. **qryDailyTransactions** \- Totals query that groups the data by stock code and transaction date

    SELECT 
      STOCKCODE, 
      TRANDTETME, 
      Sum(QTY1) AS DailyTotal
    FROM STK_STOCKTRANSACTION
    WHERE TRANSACTIONTYPE Not In (""INC CP"", ""CSTCNG"", ""DEC CP"", ""CPDIFF"")
    GROUP BY STOCKCODE, TRANDTETME;

2. **qryDates** \- Generates date series

    SELECT DateAdd(""d"", -([n] + 1), Date()) AS TheDate
    FROM tblNumbers;

3. **qryStocks** \- Generates unique list of stock codes

    SELECT DISTINCT STOCKCODE
    FROM STK_STOCKTRANSACTION;

4. **qrySalesRate** \- Final query to run the calculation

    SELECT 
        d.TheDate, 
        s.STOCKCODE,
        (SELECT Sum(dt.DailyTotal)
         FROM qryDailyTransactions AS dt
         WHERE dt.STOCKCODE = s.STOCKCODE 
           AND dt.TRANDTETME <= d.TheDate
        ) AS RunningBalance
    FROM qryStocks AS s, qryDates AS d
    ORDER BY s.STOCKCODE, d.TheDate;

**RESULT:** qrySalesRate never completes and just crashes. I assume the calculation is too complex and this is where I get stuck. I have tried limiting the qryStocks query to one stock code, but it still cannot complete the calculation.

Am I trying an impossible calculation? Is there any way I can optimize this calculation to get it to complete?

",0,1,NogIemand,2025-03-07 10:37:43,https://www.reddit.com/r/dataengineering/comments/1j5kmt6/any_advice_on_a_complicated_calculation/,False,False,False,False
1j5jckx,Using Pandas for data analysis in ComfyUI,"Hi,  
Does anyone here use Pandas for data analysis and also work with ComfyUI for image generation, either as a hobby or for work?  

I created a set of Pandas wrapper nodes that allow users to leverage Pandas within ComfyUI through its intuitive GUI nodes. For example, users can load CSV files and perform joins directly in the interface. This package is meant for structured data analysis, not for analyzing AI-generated images, though it does support manipulating PyTorch tensors.  

I love ComfyUI and appreciate how it makes Stable Diffusion accessible to non-engineers, allowing them to customize workflows easily. I believe my extension could help non-programmers use Pandas via familiar ComfyUI interface.  

My repo is here: [https://github.com/HowToSD/ComfyUI-Data-Analysis](https://github.com/HowToSD/ComfyUI-Data-Analysis).  
List of nodes is documented here: [https://github.com/HowToSD/ComfyUI-Data-Analysis/blob/main/docs/reference/node_reference.md](https://github.com/HowToSD/ComfyUI-Data-Analysis/blob/main/docs/reference/node_reference.md).  

Since ComfyUI has many AI-related extensions, users can integrate their Pandas analysis into AI-driven workflows.  

I'd love to hear your feedback!  

I posted a similar message on r/dfpandas a while ago, so apologies if you've already seen it.",0,1,HowToSD,2025-03-07 09:06:38,https://www.reddit.com/r/dataengineering/comments/1j5jckx/using_pandas_for_data_analysis_in_comfyui/,False,False,False,False
1j5hnr7,Synapse DW (dedicated sql pools) : How to Automatically Create Monthly Partitions in an Incremental Load Table?,"Hi all,

We have a table where we plan to create partitions based on a month_year column (YYYYMM). This table follows an insert-only incremental load approach.

I need help figuring out how to automatically create a new partition when data for the next month is inserted.

Daily Inserts: ~2 million records

Total Records: ~500 million


What would be the best approach to achieve this? Any recommendations on partitioning strategies or automation would be greatly appreciated.
",0,1,Engineer2309,2025-03-07 07:00:18,https://www.reddit.com/r/dataengineering/comments/1j5hnr7/synapse_dw_dedicated_sql_pools_how_to/,False,False,False,False
1j5anxv,Ververica Academy Live! Master Apache FlinkÂ® in Just 2 Days,"**Limited Seats Available for Our Expert-Led Bootcamp Program**

Hello data engineering community!Â I wanted to share an opportunity that might interest those looking to deepen their **Apache Flink**^(Â®) expertise. TheÂ [Ververica Academy](https://www.ververica.academy/app)Â is hosting successful Bootcamp in several cities over the coming months:

* Warsaw, Poland: 6-7 May 2025Â 
* Lima, Peru: 27-28 May 2025Â 
* New York City: 3-4 June 2025Â 
* San Francisco: 24-25 June 2025Â 

This is a 2-day intensive program specifically designed for those with 1-2+ years of Flink experience. The curriculum covers practical skills many of us work with daily - advanced windowing, state management optimization, exactly-once processing, and building complex real-time pipelines.

Participants will get hands-on experience with real-world scenarios using Ververica technology.If you've been looking to level up your Flink skills, this might be worth exploring. For all the details clickÂ [here](https://www.ververica.com/events/academy)!

We have group discounts for teams and organizations too!

As always if you have any questions, please reach out.

\*I work for Ververica",0,0,wildbreaker,2025-03-07 00:28:02,https://www.reddit.com/r/dataengineering/comments/1j5anxv/ververica_academy_live_master_apache_flink_in/,False,False,False,False
1j54id5,Different ways of working with SQL Databases in Go,,0,0,der_gopher,2025-03-06 19:59:06,https://packagemain.tech/p/different-ways-of-working-with-sql?share,False,False,False,False
1j50al9,Postgres to ClickHouse: Data Modeling Tips V2,,0,0,saipeerdb,2025-03-06 17:04:36,https://clickhouse.com/blog/postgres-to-clickhouse-data-modeling-tips-v2,False,False,False,False
1j5ky3d,Summary totals,"Hi there we have some big transaction tables in our operational data model.

The system was built when our data volumes where much smaller but with the growth of the company we have now at several billion rows. 

The most common read request on the data is sum of all transactions for xxx customer and this read has become quite slow.  Itâ€™s done tens of thousands of times a day. 

Partitioning is typically done on transaction date but given this read doesnâ€™t care about the date it just wants the sum of transactions for a customer this doesnâ€™t help. 

A couple of questions? 

Is partitioning on customer id viable/good practice is its also an int and couple therefore be grouped into ranges? 

Is creating a summary table with the total current balance by customer viable? Is that good practice it? Or perhaps stamping the total on the record on insert My gut feel is that itâ€™s not especially as it would need updating whenever new transactions were added? 

Any other suggestions ? 

The table is simple .. customerid, transaction amount, date, transaction type id.  So itâ€™s not wide and doenst have any text heavy fields. 

",0,4,Leiela123,2025-03-07 10:59:51,https://www.reddit.com/r/dataengineering/comments/1j5ky3d/summary_totals/,False,False,False,False
1j50uve,Distributed Systems without Raft (part 1),,0,0,david-delassus,2025-03-06 17:28:13,https://david-delassus.medium.com/distributed-systems-without-raft-part-1-a6b0b43db7ee,False,False,False,False
1j5drzj,An Open Source DuckDB Alternative,[https://github.com/SPLWare/esProc/wiki/We-Built-an-Open-Source-DuckDB-Alternative](https://github.com/SPLWare/esProc/wiki/We-Built-an-Open-Source-DuckDB-Alternative),0,25,Vast_Lab8278,2025-03-07 03:06:26,https://www.reddit.com/r/dataengineering/comments/1j5drzj/an_open_source_duckdb_alternative/,False,False,False,False
1j54657,Is it possible to become a data engineer with no experience and an unrelated degree?,"Hi everyone,

I hope this is okay to post here. Iâ€™ve been thinking a lot about trying to become a data engineer, but Iâ€™m feeling pretty overwhelmed and unsure if itâ€™s even possible for someone like me to break into this field.

I have a degree, but itâ€™s in accounting â€” completely unrelated to tech or data. I also donâ€™t have any work experience in this area, so I feel like Iâ€™m starting from zero. Despite that, Iâ€™ve always been really interested in coding and have been considered pretty tech-savvy by people around me. I genuinely love learning new things, and I donâ€™t mind putting in the time to take courses, read books, or watch videos to build my skills.

I guess what Iâ€™m wondering is: is it still possible to become a data engineer without a tech-related degree? If I focus on taking courses, getting certifications, and building a portfolio, could that realistically lead to a job? Or is a degree in computer science or a similar field pretty much necessary?

I donâ€™t mind if it takes a long time â€” Iâ€™m patient and willing to put in the work. Given where I am in life right now, I feel like this path could be a great fit for me, but I just donâ€™t know if itâ€™s too late to start or if I even have a chance without formal education in this area.

I would truly appreciate any advice, resources, or insights anyone is willing to share. And thank you so much in advance to anyone who takes the time to respond â€” I really, really appreciate it. Also, apologies if anything I said came across the wrong way or if this kind of post isnâ€™t appropriate for the subreddit. I just feel a bit lost and wanted to reach out to people who know this field better than I do.

Thank you again so much!
",0,30,silly_goose4evah,2025-03-06 19:44:36,https://www.reddit.com/r/dataengineering/comments/1j54657/is_it_possible_to_become_a_data_engineer_with_no/,False,False,False,False
