id,title,selftext,score,num_comments,author,created_utc,url,over_18,edited,spoiler,stickied
1j8sflb,BEWARE Redshift Serverless + Zero-ETL,"Our RDS database finally grew to the point where our Metabase dashboards were timing out. We considered Snowflake, DataBricks, and Redshift and finally decided to stay within AWS because of familiarity. Low and behold, there is a Serverless option! This made sense for RDS for us, so why not Redshift as well? And hey! There's a Zero-ETL Integration from RDS to Redshift! So easy!

And it is. Too easy. Redshift Serverless defaults to 128 RPUs, which is very expensive. And we found out the hard way that **the Zero-ETL Integration causes Redshift Serverless' query queue to nearly always be active**, because it's constantly shuffling transitions over from RDS. Which means that nice auto-pausing feature in Serverless? Yeah, it almost never pauses. We were spending over $1K/day when our target was to start out around that much per MONTH.

So long story short, we ended up choosing a smallish Redshift on-demand instance that costs around $400/month and it's fine for our small team.

My $0.02 -- **never use Redshift Serverless with Zero-ETL**. Maybe just never use Redshift Serverless, period, unless you're also using Glue or DMS to move data over periodically.",122,47,kangaroogie,2025-03-11 14:46:08,https://www.reddit.com/r/dataengineering/comments/1j8sflb/beware_redshift_serverless_zeroetl/,False,False,False,False
1j8uwyp,"We mapped 144 articles across 100 sources to uncover U.S. Dependence on Chinese Critical Minerals, Key Reserves in Canada, Greenland & Ukraine, and Trumpâ€™s Foreign Policy. [OC]",,66,10,boundless-discovery,2025-03-11 16:30:52,https://i.redd.it/5qcbc9ox73oe1.png,False,False,False,False
1j8r55t,Linting dbt metadata: dbt-score,"I am using dbt for 2 years now at my company, and it has greatly improved the way we run our sql scripts! Our dbt projects are getting bigger and bigger, reaching almost 1000 models soon. This has created some problems for us, in terms of consistency of metadata etc.

Because of this, I developed an open-source linter called [dbt-score](https://github.com/PicnicSupermarket/dbt-score). If you also struggle with the consistency of data models in large dbt projects, this linter can really make your life easier! Also, if you are a dbt enthousiast, like programming in python and would like to contribute to open-source; do not hesitate to join us on Github!

It's very easy to get started, just follow the instructions here: [https://dbt-score.picnic.tech/get\_started/](https://dbt-score.picnic.tech/get_started/)

*Sorry for the plug, hope it's allowed considering it's free software.*",37,4,Ok_Competition550,2025-03-11 13:47:29,https://www.reddit.com/r/dataengineering/comments/1j8r55t/linting_dbt_metadata_dbtscore/,False,False,False,False
1j8zahv,Are you using Apache iceberg?,"Currently starting a migration to Apache iceberg to be used with Athena and Redshift. 

I am curious to know who is using this in production. Have you experienced any issues? What engines are you using? ",25,11,exact-approximate,2025-03-11 19:29:19,https://www.reddit.com/r/dataengineering/comments/1j8zahv/are_you_using_apache_iceberg/,False,False,False,False
1j8znm9,Review my project,"I recently did a project on Data Engineering with Python. The project is about collecting data from a streaming source, which I simulated based on industrial IOT data. The setup is locally done using docker containers and Docker compose. It runs on MongoDB, Apache kafka and spark. 

One container simulates the data and sends it into a data stream. Another one captures the stream, processes the data and stores it in MongoDB. The visualisation container runs a Streamlit Dashboard, which monitors the health and other parameters of simulated devices. 

  
I'm a junior-level data engineer in the job market and would appreciate any insights into the project and how I can improve my data engineering skills.

Link: [https://github.com/prudhvirajboddu/manufacturing\_project](https://github.com/prudhvirajboddu/manufacturing_project)",19,4,JrDowney9999,2025-03-11 19:44:08,https://www.reddit.com/r/dataengineering/comments/1j8znm9/review_my_project/,False,False,False,False
1j8tykx,"My (end of) 2024 ""data"" and/or ""analyst"" job search results","I should say, I am still receiving rejections for some of these. The most ""days to reply"" is currently 157.

I cast as wide a net as possible, including multiple geographic regions. I ended up going from data engineer at an F500 non-tech company to data engineer at a non-F500 tech company.",12,8,jeff_kaiser,2025-03-11 15:51:24,https://imgur.com/a/olkhKik,False,False,False,False
1j8r9v7,Seeking Advice on Optimizing ETL Pipeline with SQLAlchemy,"Hello, Data Engineering community! I'mÂ seeking advice on my ETL pipeline architecture. IÂ want to make sure I'm heading in the right direction beforeÂ investing more time into development.

# Current Setup

* SQL-based ETL pipeline withÂ scripts executed via cron scheduler

* Heavy reliance on PostgreSQL materialized views for transformation and data enrichment

* TheseÂ materialized views pre-compute complex joinsÂ and aggregations between tables

* Data volume: Approximately 60 million rows inÂ the main 2 tables that contain spatial data

* Current transformations primarilyÂ involve enriching tables with additional fields from otherÂ materialized views

# Pain Points

* SQL scripts are becoming difficult to maintain and reasonÂ about

* Limited flexibility for handling diverseÂ data sources (currently PostgreSQL, but expectingÂ CSV files and potentially a graph databaseÂ in the future)

* PoorÂ visibility into processing steps and lack of properÂ auditing

* No standardized error handling or logging

* Difficult to implement data quality checks

# Proposed Approach

I'm considering a transition to Python-based ETL using SQLAlchemy CoreÂ (not ORM) to:

1. Implement proper auditing (tracking data lineage, processing times, etc.)
2. Create a more flexible pipelineÂ that can handle various data sources
3. Standardize the approach for creating new pipelines
4. Improve error handling and logging
5. Apache airflow will be used for orchestration

# Questions

1. Performance Concerns: With datasets ofÂ 10s of millions rows, isÂ SQLAlchemy Core a viable alternative to materialized views forÂ transformation logic? Or should I keep the heavyÂ lifting in SQL
2. PandasÂ Viability: Is Pandas completelyÂ off the table for datasets of this size, or are there techniques (chunking,Â dask, etc.) that make it feasible
3. BestÂ Practices: What are the best practices for implementing auditingÂ and data lineage in an ETL pipeline?
4. Hybrid Approach: Would a hybrid approach work better - keeping some transformations in SQL (views/functions) whileÂ handling orchestration and simpler transformations in Python?

# Technical Context

* Database: PostgreSQLÂ (production will include both Oracle and Postgre as sources)

* Infrastructure: On-premises servers

* Current ETL processÂ runs daily

* I come from aÂ Java backend development background with some Python and Pandas experience

* New to formal data engineering but eager to followÂ best practices

I appreciate any insights, resources, or alternative approaches youÂ might suggest. Thanks in advance for your help!",12,5,I_Bang_Toasters,2025-03-11 13:53:26,https://www.reddit.com/r/dataengineering/comments/1j8r9v7/seeking_advice_on_optimizing_etl_pipeline_with/,False,False,False,False
1j92x7u,Best Automated Approach for Pulling SharePoint Files into a Data Warehouse Like Snowflake?,"Hey everyone,

At my company different teams across multiple departments are using SharePoint to store and share files. These files are spread across various team folders libraries and sites which makes it tricky to manage and consolidate the data efficiently.

We are using Snowflake as our data warehouse and Power BI along with other BI tools for reporting. Ideally we want to automate getting these SharePoint files into our database so they can be properly used (by this, I mean used downstream in reporting in a centralized fashion).

Some Qs I have:

- What is the best automated approach to do this?

- How do you extract data from multiple SharePoint sites and folders on a schedule?

- Where should the data be centralized before loading it into Snowflake?

- How do you keep everything updated dynamically while ensuring data quality and governance?


If you have set up something similar I would love to hear what worked or did not work for you. Any recommended tools best practices or pitfalls to avoid?

Thanks for the help!

",10,19,analytical_dream,2025-03-11 21:59:37,https://www.reddit.com/r/dataengineering/comments/1j92x7u/best_automated_approach_for_pulling_sharepoint/,False,False,False,False
1j8qg8w,"Bufstream, Buf's Kafka replacement, passes multi-region 100GiB/300GiB read/write benchmark","Last week, we (Buf) subjected Bufstream to a multi-region benchmark on GCP emulating some of the largest known Kafka workloads. It passed, while also supporting active/active write characteristics and zero lag across regions.


With multi-region Spanner plugged in as its backing metadata store, Kafka deployments can offload all state management to GCP with no additional operational work.


Full details are available on our blog:


[https://buf.build/blog/bufstream-multi-region](https://buf.build/blog/bufstream-multi-region)",8,0,jrinehart-buf,2025-03-11 13:14:26,https://www.reddit.com/r/dataengineering/comments/1j8qg8w/bufstream_bufs_kafka_replacement_passes/,False,False,False,False
1j8u93o,"Do you have a dev, staging, prod MWAA environment? Or dev, staging, prod DAGs in one shared environment?","
Trying to figure out what the right call is hereâ€”or even whatâ€™s generally used. I have an AWS-based data platform established that needs orchestration. It implements resource branchingâ€”so I have dev, staging, and prod pipelines and lakehouses.

I could create an MWAA environment for every branch, though this is much more expensive (MWAA would become one of my biggest costs). I could also create one environment that works like CI/CD pipelines and simply changes config values based on what branch itâ€™s supposed to be interacting with.

Whatâ€™s usually the approach you see with implementing MWAA environments? One environment per branch?

Edit: For clarity, I realize my title presents a third scenario that I didnâ€™t bring up in the post body. Altogether these are the options I see:

1. One MWAA per branch
2. One MWAA, a dag per branch
3. One MWAA, a single dag thatâ€™s dynamic. Config values indicate branch.",7,6,DuckDatum,2025-03-11 16:03:45,https://www.reddit.com/r/dataengineering/comments/1j8u93o/do_you_have_a_dev_staging_prod_mwaa_environment/,False,False,False,False
1j8me09,Glue Database to Postgres,"Does anyone know if it's possible to somehow transactionaly replicate Iceberg-based Glue database into Postgres without using a staging area on Postgres itself?

We have a need to supply a backend of an application with only the latest data from our ETL pipeline, but we also want to start building up history for future requirements, so we want to switch from ingesting our data directly into Postgres to have a sink in Iceberg Glue database first.
But this raises an issue - how do we get multi-table transactionality when pushing data from Glue database into Postgres?

Before, on Postgres, we achieved this by dumping all tables into a ""staging"" area and running a stored procedure that merges staging tables with ""live"" tables transactionaly. We were hoping to avoid doing this if we switch to Glue-based sink, but now we are not so sure. ",7,2,Kojimba228,2025-03-11 08:55:47,https://www.reddit.com/r/dataengineering/comments/1j8me09/glue_database_to_postgres/,False,False,False,False
1j9ajew,production-grade RAG AI locally with rlama v0.1.26,"Hey everyone, I wanted to share a cool tool that simplifies the whole RAG (Retrieval-Augmented Generation) process! Instead of juggling a bunch of components like document loaders, text splitters, and vector databases, **rlama** streamlines everything into one neat CLI tool. Hereâ€™s the rundown:

* **Document Ingestion & Chunking:** It efficiently breaks down your documents.
* **Local Embedding Generation:** Uses local models via Ollama.
* **Hybrid Vector Storage:** Supports both semantic and textual queries.
* **Querying:** Quickly retrieves context to generate accurate, fact-based answers.

This local-first approach means you get better privacy, speed, and ease of management. Thought you might find it as intriguing as I do!

# Step-by-Step Guide to Implementing RAG withÂ rlama

# 1. Installation

Ensure you have [Ollama](https://ollama.ai/) installed. Then, run:

    curl -fsSL https://raw.githubusercontent.com/dontizi/rlama/main/install.sh | sh

Verify the installation:

    rlama --version

# 2. Creating a RAGÂ System

Index your documents by creating a RAG store (hybrid vector store):

    rlama rag <model> <rag-name> <folder-path>

For example, using a model like `deepseek-r1:8b`:

    rlama rag deepseek-r1:8b mydocs ./docs

This command:

* Scans your specified folder (recursively) for supported files.
* Converts documents to plain text and splits them into chunks (default: moderate size with overlap).
* Generates embeddings for each chunk using the specified model.
* Stores chunks and metadata in a local hybrid vector store (in `~/.rlama/mydocs`).

# 3. Managing Documents

Keep your index updated:

* **Add Documents:**rlama add-docs mydocs ./new\_docs --exclude-ext=.log
* **List Documents:**rlama list-docs mydocs
* **Inspect Chunks:**rlama list-chunks mydocs --document=filename
* `rlama list-chunks mydocs --document=filename`
* **Update Model:**rlama update-model mydocs <new-model>

# 4. Configuring Chunking and Retrieval

**Chunk Size & Overlap:**  
Â Chunks are pieces of text (e.g. \~300â€“500 tokens) that enable precise retrieval. Smaller chunks yield higher precision; larger ones preserve context. Overlapping (about 10â€“20% of chunk size) ensures continuity.

**Context Size:**  
Â The `--context-size` flag controls how many chunks are retrieved per query (default is 20). For concise queries, 5-10 chunks might be sufficient, while broader questions might require 30 or more. Ensure the total token count (chunks + query) stays within your LLMâ€™s limit.

**Hybrid Retrieval:**  
Â While `rlama` primarily uses dense vector search, it stores the original text to support textual queries. This means you get both semantic matching and the ability to reference specific text snippets.

# 5. RunningÂ Queries

Launch an interactive session:

    rlama run mydocs --context-size=20

In the session, type your question:

    > How do I install the project?

`rlama`:

1. Converts your question into an embedding.
2. Retrieves the top matching chunks from the hybrid store.
3. Uses the local LLM (via Ollama) to generate an answer using the retrieved context.

You can exit the session by typing `exit`.

# 6. Using the rlamaÂ API

Start the API server for programmatic access:

    rlama api --port 11249

Send HTTP queries:

    curl -X POST http://localhost:11249/rag \
      -H ""Content-Type: application/json"" \
      -d '{
            ""rag_name"": ""mydocs"",
            ""prompt"": ""How do I install the project?"",
            ""context_size"": 20
          }'

The API returns a JSON response with the generated answer and diagnostic details.

# Recent Enhancements andÂ Tests

# EnhancedHybridStore

* **Improved Document Management:** Replaces the traditional vector store.
* **Hybrid Searches:** Supports both vector embeddings and textual queries.
* **Simplified Retrieval:** Quickly finds relevant documents based on user input.

# Document StructÂ Update

* **Metadata Field:** Now each document chunk includes a `Metadata` field for extra context, enhancing retrieval accuracy.

# RagSystem Upgrade

* **Hybrid Store Integration:** All documents are now fully indexed and retrievable, resolving previous limitations.

# Router Retrieval Testing

I compared the new version with v0.1.25 using `deepseek-r1:8b` with the prompt:

>*â€œlist me all the routers in the codeâ€*  
Â (as simple and general as possible to verify accurate retrieval)

* **Published Version on GitHub:** Â **Answer:** The code contains at least one router, `CoursRouter`, which is responsible for course-related routes. Additional routers for authentication and other functionalities may also exist. Â *(Source: src/routes/coursRouter.ts)*
* **New Version:** Â **Answer:** There are four routers: `sgaRouter`, `coursRouter`, `questionsRouter`, and `devoirsRouter`. Â *(Source: src/routes/sgaRouter.ts)*

# Optimizations and Performance Tuning

**Retrieval Speed:**

* Adjust `context_size` to balance speed and accuracy.
* Use smaller models for faster embedding, or a dedicated embedding model if needed.
* Exclude irrelevant files during indexing to keep the index lean.

**Retrieval Accuracy:**

* Fine-tune chunk size and overlap. Moderate sizes (300â€“500 tokens) with 10â€“20% overlap work well.
* Use the best-suited model for your data; switch models easily with `rlama update-model`.
* Experiment with prompt tweaks if the LLM occasionally produces off-topic answers.

**Local Performance:**

* Ensure your hardware (RAM/CPU/GPU) is sufficient for the chosen model.
* Leverage SSDs for faster storage and multithreading for improved inference.
* For batch queries, use the persistent API mode rather than restarting CLI sessions.

# Next Steps

* **Optimize Chunking:** Focus on enhancing the chunking process to achieve an optimal RAG, even when using small models.
* **Monitor Performance:** Continue testing with different models and configurations to find the best balance for your data and hardware.
* **Explore Future Features:** Stay tuned for upcoming hybrid retrieval enhancements and adaptive chunking features.

# Conclusion

`rlama` simplifies building local RAG systems with a focus on confidentiality, performance, and ease of use. Whether youâ€™re using a small LLM for quick responses or a larger one for in-depth analysis, `rlama` offers a powerful, flexible solution. With its enhanced hybrid store, improved document metadata, and upgraded RagSystem, itâ€™s now even better at retrieving and presenting accurate answers from your data. Happy indexing and querying!

Github repo: [https://github.com/DonTizi/rlama](https://github.com/DonTizi/rlama)

website: [https://rlama.dev/](https://rlama.dev/)

X: [https://x.com/LeDonTizi/status/1898233014213136591](https://x.com/LeDonTizi/status/1898233014213136591)",6,1,DonTizi,2025-03-12 04:01:30,https://www.reddit.com/r/dataengineering/comments/1j9ajew/productiongrade_rag_ai_locally_with_rlama_v0126/,False,False,False,False
1j8s325,Scaling Beyond Postgres: How to Choose a Real-Time Analytical Database,,7,0,sspaeti,2025-03-11 14:30:43,https://www.rilldata.com/blog/scaling-beyond-postgres-how-to-choose-a-real-time-analytical-database,False,False,False,False
1j8y5d7,Any experience with AWS Sagemaker Lakehouse?,"Basically allows you to create iceberg-compatible Catalogs for the different data sources (s3, redshift, snowflake, etc). Consumers use these in queries or write to new tables.

I think I understood that right. 

They've had Lakehouse [blog posts since 2021](https://aws.amazon.com/blogs/big-data/build-a-lake-house-architecture-on-aws/), so trying to understand what is the main selling point or improvement here



  
\* [Simplify analytics and AI/ML with new Amazon SageMaker Lakehouse | AWS News Blog](https://aws.amazon.com/blogs/aws/simplify-analytics-and-aiml-with-new-amazon-sagemaker-lakehouse/)

\* [Simplify data access for your enterprise using Amazon SageMaker Lakehouse | AWS Big Data Blog](https://aws.amazon.com/blogs/big-data/simplify-data-access-for-your-enterprise-using-amazon-sagemaker-lakehouse/)

",3,1,gman1023,2025-03-11 18:42:26,https://www.reddit.com/r/dataengineering/comments/1j8y5d7/any_experience_with_aws_sagemaker_lakehouse/,False,False,False,False
1j8mwcq,Cloud Composer 2 and KubernetesPodOperator,"Hi there, anyone experienced in using these two in conjunction and specifying custom resource limits?

I am literally following the docs [here](https://cloud.google.com/composer/docs/composer-2/use-kubernetes-pod-operator#composer-2-kpo-access-project-resources) and have specified custom ResourceRequirements, to no avail. 

My pods log only a fraction of the specified resource limit as available and return OOMs. 

Everything else works as expected.",4,1,BanaBreadSingularity,2025-03-11 09:34:59,https://www.reddit.com/r/dataengineering/comments/1j8mwcq/cloud_composer_2_and_kubernetespodoperator/,False,False,False,False
1j8z2m8,How do you store mocked data for testing?,"Iâ€™m curious how others store mocked data for testing purposes. Iâ€™ve built a bunch of mocked tables for silver and gold layers. Iâ€™ve created them as fixtures and populate the spark data frame with data stored in json. The json is a little annoying to work especially when creating new tests because you canâ€™t easily compare rows and have to look through the ison. 

Iâ€™m curious what others use? Store data in csv and create data frames that way? Something else? ",3,5,justanator101,2025-03-11 19:20:25,https://www.reddit.com/r/dataengineering/comments/1j8z2m8/how_do_you_store_mocked_data_for_testing/,False,False,False,False
1j8yucw,Does Delta Table Z-Order and Optimise load all data or just the most recent based on insertionorder?,"I am working on project where I get time series data on change from a different database and dumping via delta-rs to a azure storage account data lake gen 2. I am currently running Z-Order and vaccum every 20 iterations and then resuming. Main question was does z-order load all data for optimising? Currently the data isn't that much but soon over time it will grow very large. 
 
My Schema is given below and I z-order based on all of those columns.
 
{""id"":int,""name"":string,""timestamp"":datetime.datetime
}
 
Also are optimise operations acid complaint? What if I optimise via another job while I'm still appending every minute to the table.",3,2,tecedu,2025-03-11 19:11:04,https://www.reddit.com/r/dataengineering/comments/1j8yucw/does_delta_table_zorder_and_optimise_load_all/,False,False,False,False
1j8to8h,Critique my data platform recommendation for analytics team,"Iâ€™m tasked with improving the data platform for our small analytics team (20+6 analysts and 2 data scientists) within the company. Currently, we aggregate real-time data from small XML files into a large flat table with a massive number of columns for a single transaction. The XMLs contain all the relevant details of each transaction, and we break them into logical chunks before consolidating everything into one flat table, which is stored in Oracle. We have some dimensions table such as calendar, ZIPCode, Client.

Additionally, the XML and parsed data is stored in on-prem Hadoop storage, which then loads into Oracle every hour.

# Current Problems:

1. **Data Unmanageability**: The sheer size of the data makes it unmanageable. We can only store 1 year of data in the flat table; older partitions are moved into archive tables.
2. **Query Performance**: Most analysis focuses on recent data, so at least the queries on this subset of data are somewhat manageable.
3. **Data Duplication**: Large volumes of data are being dumped onto laptops and VMs for analysis, which is inefficient.
4. **Slow Dashboards**: Tableau dashboards that directly use Oracle are slow to refresh.
5. **Data Science Workflow**: Data scientists run queries and Python code on large VMs to churn through models, but the setup is less efficient.

# Current Workflow:

1. Pull data using SQLs and then use Tableau and Excel for analysis.
2. Some Tableau dashboards are connected to Oracle, leading to long refresh times.
3. Some SQL queries are executed daily/weekly on a VM to generate data dumps, which are then consumed by Tableau or Excel.
4. Data scientists run queries and Python code on large VMs to build models.
5. We have around 2 data scientists, 6 data analysts and around 20 operational analysts.
6. Skillset: data scientists are python and sql savvy. And analysts are comfortable on SQL.

# Proposed Solution:

**Phase 1:**

1. **Copy Data to S3**: Move the raw text files into **Amazon S3**.
2. **Hourly Glue Job**: Create an hourly **AWS Glue** job to read the S3 files and convert them into **Parquet** format, partitioned by **YYYY-MM-DD HH**.
3. **Load Data into Snowflake**: Use **Snowpipe** or an **AWS Lambda** job to automatically copy the Parquet files into a single large table (`main.OBT`) in **Snowflake**, within `schemaA`.
4. Copy dimension tables from oracle to snowflake once a day.
5. **Analyst Querying**: Analysts will create queries to read from `main.OBT` and can write their results to smaller tables in a separate **analyst schema**. Queries can be orchestrated with **Airflow MWAA**.
6. **Data Science Workflow**: Parquet files can be read by the data science team either by continuing to use VMs or transitioning to **Snowpark** for more integrated processing

**Phase 2:**

1. **Copy XMLs to S3**: Move the raw XML files to **S3**.
2. **Parse XMLs with Glue**: Create **AWS Glue** ETL jobs to parse the XML files into smaller **Parquet** files.
3. **Load Data into Snowflake**: Load the smaller **Parquet** files into **Snowflake** tables.
4. **Combine into Main Table**: Combine the smaller tables into the `main.OBT` table for easy access by analysts.

**Phase 3:**

1. **Create Views & Aggregated Tables**: For common use cases, create **views**, **materialized views**, and **aggregated tables** on top of `main.OBT`.
2. **Analyst Schema**: Analysts will use their schema to build the data they need, and if certain tables or views prove valuable, they can be incorporated into the **main schema**. The advantage to promote tables into main schema would be data quality checks and data engineers would own maintaining the tables going forward.
3. Retire the old jobs that build Dimension tables and build new jobs on cloud.

  
Edited:

1. I am leaning towards not using DBT atleast in phase1 to keep the learning curve low. I don't want one more thing you will have to learn to use the new system.

",3,17,ask_can,2025-03-11 15:39:19,https://www.reddit.com/r/dataengineering/comments/1j8to8h/critique_my_data_platform_recommendation_for/,False,False,False,False
1j8ou1f,Data copy from HDFS to MinIO regularly,"Hello Friends,

There is an application that was developed 5 year ago, and this application processes 10GB of binary data per hour using MapReduce and generates 100GB of data, which is then written to the HDFS file system.

My goal is to move a portion of the processed data (approximately 25%) to a MinIO cluster that I plan to use as new object storage. I want this operation to be repeated every time new data is added to the HDFS cluster.

What kind of solution would you suggest to complete this task? Additionally, I would like to remind you that I have requirements related to monitoring the pipeline I am developing.

Thank you.",3,3,adidaf14,2025-03-11 11:46:55,https://www.reddit.com/r/dataengineering/comments/1j8ou1f/data_copy_from_hdfs_to_minio_regularly/,False,False,False,False
1j8mlab,Optimizing Flink to Process Time-Series Data,"Hi all. I have a Kafka stream that produces around 5 million records per minute and has 50 partitions, Each Kafka record, once deserialized is a json record, where the values for keys 'a','b', and 'c' rpepresent the unique machine for the time series data, and value of key 'data\_value' represent the float value of the record.  All the records in this stream are coming in order. I am using PyFlink to compute specific 30-second aggregations on certain machines within my.

I also have another config kafka stream, where each element in the stream represents the latest machines to monitor. I join this stream with my time-series kafka stream using a broadcast process operator, and filter down records from my raw time-series kafka stream to only ones from relevant machines in the config kafka stream. 

Once I filter down my records, I then key my filtered stream by machine (keys 'a','b', and 'c'  for each record), and call my Keyed Process Operator. In my Process function, I trigger a timer event in 30 seconds once the first record is received and then append all the subsequent  time-series values in my process value state (I set it up as list). Once the timer is triggered, I compute multiple aggregation functions on the time-series values in my value state.

I'm facing a lot of latency issues with the way I have currently structured my PyFlink job.  I currently have 85 threads, with 5 threads per task manager, and each task manager using 2 CPU and 4 GB RAM. This works fine when in my config kafka stream has very few machines, and I filter my raw Kafka stream from 5 million per minute to 70k records per minute. However, when more machines get added to my config Kafka stream, and I start filtering less records, the latency really starts to pile up, to the point where the event\_time and processing\_time of my records are almost hours apart after running for a few hours even close. My theory is it's due to keying my filtered stream since I've heard that can be expensive.

  
I'm wondering if there is any chances for optimizing my PyFlink pipeline, since I've heard Flink should be able to handle way more than 5 million records per minute. In an ideal world, even if no records are filtered from my raw time-series kafka stream, I want my PyFlink pipeline to still be able to process all these records without huge amounts of latency piling up, and without having to explode the resources. 

In short, the steps in my Flink pipeline after receiving the raw Kafka stream are:

* Deserialize record
* Join and filter  on Config Kafka Stream using Broadcast Process Operator
* Key by fields 'a','b', and 'c' and call Process Function to execute aggregation in 30 seconds

Is there any options for optimization in the steps in my pipeline to mitigate latency, without having to blow up resources. Thanks.



",3,4,raikirichidori255,2025-03-11 09:11:16,https://www.reddit.com/r/dataengineering/comments/1j8mlab/optimizing_flink_to_process_timeseries_data/,False,False,False,False
1j9af4c,De on an AI team,Hello everyone! Iâ€™m a recent new grad who secured a job at big tech company as DE. I was told my team works primarily on recommendation systems and that Iâ€™ll be using a bit of PyTorch aswell as some loss bucket analysis. I was wondering if anyone could give me anymore insights on what I should expect or resources to read up on. Thank you!! ,2,1,Yeat_enjoyer,2025-03-12 03:54:36,https://www.reddit.com/r/dataengineering/comments/1j9af4c/de_on_an_ai_team/,False,False,False,False
1j8zo7x,Help needed for Machine Learning Dagster use-case,"I am trying a PoC with Dagster where I would use it for Computer vision Data pipeline. If it works fine, we will extend its use cases, but currently I need the best way to utilise dagster for my use-case. 

A simplified version of use-case would be, where I have some annotated Object detection data in some standardized format. That is I would have one directory containing images and one directory containing annotated bounding box information in some format. So the next step might just be changing the format and dumping the data to a new directory.

So essentially it's just Format A --> Format B where each file from source directory is processed and stored to destination directory.
But mainly everytime someone dumps a file to Source Directory the processed file in directory B should be materialized. I would like dagster to list all the successful and failed files so that I can backfill them later.

My question how to best design this with Dagster concepts. From what I have read is the best way might be to use Partitioned Asset, especially the Dynamic ones. They seem perfect but the only issue seems the soft limit of 25000, since my use case can contain lakhs of files which might be dumped in source directory at any moment. If Partitioned assets are the best solution how to scale them beyond the 25000 limit",2,1,Quicksilver466,2025-03-11 19:44:50,https://www.reddit.com/r/dataengineering/comments/1j8zo7x/help_needed_for_machine_learning_dagster_usecase/,False,False,False,False
1j8uri7,Hydra: Serverless Real-time Analytics on Postgres,,2,0,JHydras,2025-03-11 16:24:45,https://www.ycombinator.com/launches/N0V-hydra-serverless-realtime-analytics-on-postgres,False,False,False,False
1j8ohx3,New Fabric Course Launch! Watch Episode 1 Now!,"After the great success of my free DP-203 course (50+ hours, 54 episodes, and many students passing their exams ðŸŽ‰), I'm excited to start a brand-new journey:

ðŸ”¥ Mastering Data Engineering with Microsoft Fabric! ðŸ”¥

This course is designed to help you learn data engineering with Microsoft Fabric in-depth - covering functionality, performance, costs, CI/CD, security, and more! Whether you're a data engineer, cloud enthusiast, or just curious about Fabric, this series will give you real-world, hands-on knowledge to build and optimize modern data solutions.

ðŸ’¡ Bonus: This course will also be a great resource for those preparing for the DP-700: Microsoft Fabric Data Engineer Associate exam!

ðŸŽ¬ Episode 1 is live! In this first episode, I'll walk you through:

âœ… How this course is structured & what to expect

âœ… A real-life example of what data engineering is all about

âœ… How you can help me grow this channel and keep this content free for everyone!

This is just the beginning - tons of hands-on, in-depth episodes are on the way!

[https://youtu.be/4bZX7qqhbTE](https://youtu.be/4bZX7qqhbTE)",2,4,TybulOnAzure,2025-03-11 11:25:56,https://www.reddit.com/r/dataengineering/comments/1j8ohx3/new_fabric_course_launch_watch_episode_1_now/,False,False,False,False
1j9cvtp,Looking for some certifications or training for Analytics,"I am currently working at an E-Commerce company as an Analytics Engineer. My company has education budget for us to spend each year and I would love to spend it on some meaningful training or certification for my career. Would it be better to try and get Data Engineering or Data Science certifications? What would be some good suggestions?  I am a bit confused because I still am a Junior :) 

Thank you all in advance",0,0,Chediras,2025-03-12 06:03:13,https://www.reddit.com/r/dataengineering/comments/1j9cvtp/looking_for_some_certifications_or_training_for/,False,False,False,False
1j95x5z,Â¿CÃ³mo elegir tu primera empresa siendo Ingeniero Junior?,"Buenos dÃ­as a todos!

Estoy terminando el mÃ¡ster en Big Data & IngenierÃ­a de Datos, tambiÃ©n ya he hecho una carrera de IngenierÃ­a InformÃ¡tica. Y ahora me interesa hacer mis prÃ¡cticas del mÃ¡ster o ya conseguir mi primer trabajo. Pero tengo una duda, siendo perfil Junior...cÃ³mo puedo empezar a elegir mi primera empresa? He leÃ­do por ahÃ­ de que no recomiendan empezar por una consultorÃ­a, y yo es que ya me he postulado a una consultora  IT(mediana) y me han dicho que he quedado como una de las tres finalistas para trabajar, pero luego otra empresa me ha contactado y es una empresa grande de ingenierÃ­a que ya anteriormente hice mis prÃ¡cticas para el grado de Ing. InformÃ¡tica, pero la sede en donde lo hice el equipo de IT sÃ³lo eran 2. AsÃ­ que ahora he postulado al de Barcelona ya que ahÃ­ el equipo de Datos en IT es mucho mÃ¡s grande, pero me han dicho que no hay vacantes para trabajador pero podrÃ­an darme para prÃ¡cticas. Entonces yo siendo junior en IngenierÃ­a de Datos, Â¿debo decidirme por una consultora de tecnologÃ­a o por la otra empresa de IngenierÃ­a que es reconocida internacionalmente aunque no se dedican principalmente al sector IT? Probablemente harÃ­a prÃ¡cticas ahÃ­ como 4 o 5 meses.

No sÃ© si darle prioridad para mi curriculum y ya luego de terminar mis prÃ¡cticas postular para una mejor empresa IT, o irme directo a la consultorÃ­a y ya trabajar.

Â¿CÃ³mo empezaron ustedes?

Ando un poco perdida sobre quÃ© debo valorar mejor las condiciones para el inicio de mi vida profesional.

Cualquier opiniÃ³n serÃ­a de ayuda.

Gracias por leer hasta aquÃ­.",1,1,Sayu273,2025-03-12 00:12:00,https://www.reddit.com/r/dataengineering/comments/1j95x5z/cÃ³mo_elegir_tu_primera_empresa_siendo_ingeniero/,False,False,False,False
1j8wlm4,Bus Matrix Documentation Tools,"What tools do you guys use for DW Bus Matrix Documentation? I am using Google Sheets, but I wanted to know if there is an open source tool that are more robust for this manner.",1,1,IndependentNet5042,2025-03-11 17:39:36,https://www.reddit.com/r/dataengineering/comments/1j8wlm4/bus_matrix_documentation_tools/,False,False,False,False
1j8wkzc,FREE O'REILLY ACCESS 30-day access,Click [here ](https://learning.oreilly.com/get-learning/?code=SCALE25)to get  to the complete Oâ€™Reilly platform  including early access to the second edition of Designing Data-Intensive Applications. Code is SCALE25.,3,1,OpenWeb5282,2025-03-11 17:38:54,https://www.reddit.com/r/dataengineering/comments/1j8wkzc/free_oreilly_access_30day_access/,False,False,False,False
1j8x1b7,Announcing Flink Forward Barcelona 2025!,"https://preview.redd.it/ehjonuq4n3oe1.png?width=1200&format=png&auto=webp&s=10f4c848334b77244d077ca5642e099da5b0c9ad

# [Ververica ](https://www.ververica.com/)is excited to share details about the upcoming Flink Forward Barcelona 2025!

* **Dates**: 13-16 October 2025
* **Location**: [Fira de Barcelona MontjuÃ¯c](https://www.firabarcelona.com/)

The event will follow our successful our 2+2 day format:

* **Days 1-2**:  [Ververica Academy](https://www.ververica.academy/app) Learning Sessions
* **Days 3-4**: Conference days with keynotes and parallel breakout tracks

# Special Promotion

We're offering a limited number of early bird tickets! Sign up for pre-registration to be the first to know when they become available [here](https://www.flink-forward.org/).

**Call for Presentations will open in April** \- please share with anyone in your network who might be interested in speaking!

Feel free to spread the word and let us know if you have any questions. Looking forward to seeing you in Barcelona! 

# Don't forget, Ververica Academy is hosting four intensive, expert-led Bootcamp sessions.

https://preview.redd.it/pl8w7i46n3oe1.png?width=1200&format=png&auto=webp&s=c557ce883457540c1df1faa16483b3ee5a9a6da6

This 2-day program is specifically designed for Apache Flink users with 1-2 years of experience, focusing on advanced concepts like state management, exactly-once processing, and workflow optimization.

**Click** [**here** ](https://www.ververica.academy/app)**for information on tickets, group discounts, and more!**

Discloure: I work for Ververica",0,1,wildbreaker,2025-03-11 17:57:18,https://www.reddit.com/r/dataengineering/comments/1j8x1b7/announcing_flink_forward_barcelona_2025/,False,False,False,False
1j93ygb,Microsoft Fabric??,Hey everyone. UC Berkeley student here studying CS and cognitive science.Â I'm conducting user research on Microsoft Fabric for my Data Science class and looking to connect with people who have experience using it professionally or personally. If you have used Fabric pls dm me. Greatly appreciated!!,0,0,Playful-Safety-7814,2025-03-11 22:44:49,https://www.reddit.com/r/dataengineering/comments/1j93ygb/microsoft_fabric/,False,False,False,False
1j8x1nf,Data Analyst wants to do more DB related work?,"Hi, I've always worked as a data analyst, a little bit of everything really, but I'd like to really learn a lot about how to manage and administer a database to open up new types of work (data management is kinda boring). I don't have a particular DB in mind, but Postgres is seemingly everywhere and open sourced, and seems like the best candidate for learning a DB from the ground up.

What are the most technically challenging books you can recommend? What topics would you say would are the hardest to perform?",0,2,Oh_Another_Thing,2025-03-11 17:57:40,https://www.reddit.com/r/dataengineering/comments/1j8x1nf/data_analyst_wants_to_do_more_db_related_work/,False,False,False,False
1j96dio,New to Azure - Not sure what do I do?,"I work as a data analyst and use powerbi to import transform clean and report. I have been able to create pipelines in powerbi and provide close to real time data by using refreh option. We never had a database. Only google sheet (data source) and powerbi (analytics)

I asked my ceo for a database to be able to create a system that stores data and also provide real time analyses. Meaning when a value changes in the google sheet, it should be right away updated in the powerbi. I only had some basic understanding of ETL and thats it

Now the IT team were able to get a subscription for me that is $300 per month. It has azure database with sql server and azure data factory and sent me this email (please see below). I was not given login instructions as I already can log in but when I click databases, it asks me to select subscription (please see below)

Im hesitant to ask them for stupid questions. Can someone guide me? Or provide a video tutorial. 


",0,1,Ok-Bumblebee-8256,2025-03-12 00:33:10,https://www.reddit.com/gallery/1j96dio,False,False,False,False
1j8vtwx,Stuck in TCS for ninja profile,"Hey everyone, I recently joined TCS through the Ninja profile and was allocated to a Data Engineering (DE) project in a client-facing role. The role seems great â€” the client is good, it is also WFH and the work is interesting. Right now, I'm starting as a tester since I don't have much experience with DE yet, but I believe if I stay here for 2-3 years, I can learn a lot and build valuable skills.

However, the main concern for me is the salary â€” it's 3.36 LPA, which is honestly a bit discouraging. Before joining TCS, I was an intern at a small startup, working with Angular and Spring Boot. I joined TCS primarily for the job security, and I also expected to have a lot of free time here. But it seems like life has different plans for me.

I also know React, Node.js, and DSA, though I need to refresh my skills in these areas. If I decide to switch to a full-stack developer role, I think I could make that transition in 7-8 months. But the DE role is a bigger challenge since it's not easy for freshers to get into this field maybe I can jump to a goof package.  
I need your advice please help !!",0,4,Training_Refuse7745,2025-03-11 17:08:09,https://www.reddit.com/r/dataengineering/comments/1j8vtwx/stuck_in_tcs_for_ninja_profile/,False,False,False,False
